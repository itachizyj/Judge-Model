{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c299f9-5626-413f-9ba9-f7e92bc68d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c4849-589e-48ee-ba3d-44fd91b33cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fabd04-47f4-4e9c-9fd6-2901301239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1安装微调库\n",
    "# %%capture\n",
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# 由于Colab有torch 2.2.1，会破坏软件包，要单独安装\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "# if major_version >= 8:\n",
    "#     # 新GPU，如Ampere、Hopper GPU（RTX 30xx、RTX 40xx、A100、H100、L40）。\n",
    "#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "# else:\n",
    "#     # 较旧的GPU（V100、Tesla T4、RTX 20xx）\n",
    "#     !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc8445-2135-45d5-95a2-2fe608f9f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2加载模型\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af232f-a005-441a-9e48-b0b99db0ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 微调前测试\n",
    "alpaca_prompt = \"\"\"Below are two responses for a given task. The task is defined by the Instruction. Evaluate the responses and generate a reference answer for the task.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response 1:\n",
    "{}\n",
    "### Response 2:\n",
    "{}\n",
    "### Evaluation:\n",
    "{}\n",
    "### Reason:\n",
    "{}\n",
    "### Reference:\n",
    "{}\n",
    "\"\"\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Give three tips for staying healthy.\", # instruction\n",
    "        \"\",\n",
    "        \"1. Eat a balanced and nutritious diet.\\n2. Get regular exercise.\\n3. Get enough sleep.\", # input\n",
    "        \"1. Eat a balanced diet with plenty of fruits, vegetables, and whole grains.\\n2. Get regular physical activity, such as walking, jogging, or swimming.\\n3. Get enough sleep and practice healthy sleeping habits.\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens =128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda61a0-d3ef-45f1-a835-a65aea60c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "#4. 准备微调数据集\n",
    "EOS_TOKEN = tokenizer.eos_token # 必须添加 EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    response_1s  = examples[\"response_1\"]\n",
    "    response_2s  = examples[\"response_2\"]\n",
    "    evaluations  = examples[\"evaluation\"]\n",
    "    reasons      = examples[\"reason\"]\n",
    "    references   = examples[\"reference\"]\n",
    "    texts = []\n",
    "    for instruction, input, response_1,response_2,evaluation,reason,reference in zip(instructions, inputs, response_1s,response_2s,evaluations,reasons,references):\n",
    "        # 必须添加EOS_TOKEN，否则无限生成\n",
    "        text = alpaca_prompt.format(instruction, input, response_1,response_2,evaluation,reason,reference) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"Lemon01/test_data\", split = \"train\")\n",
    "dataset = load_dataset('json', data_files='after_train.json')\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92cb39-6e5e-4531-a547-65a4f061f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26c02e-6c43-472e-bb35-a0f8238fa755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. 设置训练参数\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, #  建议 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\", # 检查点，长上下文度\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # 可以让短序列的训练速度提高5倍。\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 500,  # 微调步数\n",
    "        learning_rate = 2e-4, # 学习率\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279e2aa-b6d3-44b2-b262-c5c7184c5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. 开始训练\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ff356-5aa2-4784-b620-7da70b484937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7测试微调后的模型\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"How can we reduce air pollution?\", # instruction\n",
    "        \"\",\n",
    "        \"One way to reduce air pollution is to reduce the burning of fossil fuels. This can be done by switching to renewable energy sources such as solar, wind, and geothermal. Another way to reduce air pollution is to reduce the use of vehicles. This can be done by encouraging the use of public transportation, carpooling, and electric vehicles. Finally, it is important to reduce waste and recycle materials whenever possible\", # input\n",
    "        \"One way to reduce air pollution is to reduce the use of fossil fuels and switch to renewable energy sources such as solar, wind, and hydropower. This can be done by investing in energy-efficient appliances, switching to electric vehicles, and encouraging the use of renewable energy sources.\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "        \"\",\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens =128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1014e-50c7-42ae-9491-2be5faa808a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8保存LoRA模型\n",
    "model.save_pretrained(\"autodl-tmp/lora_model\") # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 在线保存到hugging face，需要token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b0727-ad4f-400d-b9a7-74373f047c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存tokenizer\n",
    "tokenizer.save_pretrained(\"autodl-tmp/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adc054-a588-4f96-89ea-6189c9a87808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.from_pretrained(model,\"autodl-tmp/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b15dd-490f-4ab7-8277-9632fa2e3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9合并模型并量化成4位gguf保存\n",
    "model.save_pretrained_gguf(\"autodl-tmp/model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "#model.save_pretrained_merged(\"outputs\", tokenizer, save_method = \"merged_16bit\",) #合并模型，保存为16位hf\n",
    "#model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\") #合并4位gguf，上传到hugging face(需要账号token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
