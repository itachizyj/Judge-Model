{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c4849-589e-48ee-ba3d-44fd91b33cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoDL加速huggingface连接  [JupyterLab使用]\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True,\n",
    "                        text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fabd04-47f4-4e9c-9fd6-2901301239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. 安装微调库\n",
    "# %%capture\n",
    "import torch\n",
    "\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc8445-2135-45d5-95a2-2fe608f9f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 加载模型\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af232f-a005-441a-9e48-b0b99db0ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 微调前测试\n",
    "alpaca_prompt = \"\"\"\n",
    "{}\n",
    "### Instruction:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response 1:\n",
    "{}\n",
    "### Response 2:\n",
    "{}\n",
    "### Choice & Reason:\n",
    "{}\n",
    "\"\"\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            # prompt\n",
    "            \"Below are two responses for a given task. The task is defined by the Instruction. As a judge, evaluate the responses and provide the following:\\n1. Which response do you think is better (Response 1 or Response 2)?\\n2. Explain why you think this response is better.\\nPlease ensure that both the choice and reason remain consistent upon repeated. inquiries.\",\n",
    "            # instruction\n",
    "            \"Give three tips for staying healthy.\",\n",
    "            # input \n",
    "            \"\",\n",
    "            # response 1\n",
    "            \"1. Eat a balanced and nutritious diet.\\n2. Get regular exercise.\\n3. Get enough sleep.\",\n",
    "            # response 2\n",
    "            \"1. Eat a balanced diet with plenty of fruits, vegetables, and whole grains.\\n2. Get regular physical activity, such as walking, jogging, or swimming.\\n3. Get enough sleep and practice healthy sleeping habits.\",\n",
    "            # output (choice & reason)\n",
    "            \"\",\n",
    "        )\n",
    "    ], return_tensors=\"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda61a0-d3ef-45f1-a835-a65aea60c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 准备微调数据集\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # 必须添加 EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    response_1s = examples[\"response_1\"]\n",
    "    response_2s = examples[\"response_2\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for prompt, instruction, input, response_1, response_2, evaluation, output in zip(prompts, instructions, inputs,\n",
    "                                                                                      response_1s, response_2s,\n",
    "                                                                                      outputs):\n",
    "        # 必须添加EOS_TOKEN，否则无限生成\n",
    "        text = alpaca_prompt.format(\n",
    "            prompt, instruction, input, response_1, response_2, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts, }\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='autodl-tmp/pandalm_after_preprocess_v3.json')\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26c02e-6c43-472e-bb35-a0f8238fa755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. 设置训练参数\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  #  建议 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # 检查点，长上下文度\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset['train'],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # 可以让短序列的训练速度提高5倍。\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=32,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=2100,  # 微调步数\n",
    "        learning_rate=3e-4,  # 学习率\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279e2aa-b6d3-44b2-b262-c5c7184c5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. 开始训练\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 从checkpoints继续训练\n",
    "# checkpoint_path = \"outputs/checkpoint-2000\"\n",
    "# trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ff356-5aa2-4784-b620-7da70b484937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.测试微调效果\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            # prompt\n",
    "            \"Below are two responses for a given task. The task is defined by the Instruction. As a judge, evaluate the responses and provide the following:\\n1. Which response do you think is better (Response 1 or Response 2)?\\n2. Explain why you think this response is better.\\nPlease ensure that both the choice and reason remain consistent upon repeated. inquiries.\",\n",
    "            # instruction\n",
    "            \"Give three tips for staying healthy.\",\n",
    "            # input \n",
    "            \"\",\n",
    "            # response 1\n",
    "            \"1. Eat a balanced and nutritious diet.\\n2. Get regular exercise.\\n3. Get enough sleep.\",\n",
    "            # response 2\n",
    "            \"1. Eat a balanced diet with plenty of fruits, vegetables, and whole grains.\\n2. Get regular physical activity, such as walking, jogging, or swimming.\\n3. Get enough sleep and practice healthy sleeping habits.\",\n",
    "            # output (choice & reason)\n",
    "            \"\",\n",
    "        )\n",
    "    ], return_tensors=\"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1014e-50c7-42ae-9491-2be5faa808a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. 保存模型\n",
    "# 保存LoRA模型\n",
    "model.save_pretrained(\"autodl-tmp/lora_model\")  # Local saving\n",
    "# 保存tokenizer\n",
    "tokenizer.save_pretrained(\"autodl-tmp/tokenizer\")\n",
    "# 合并模型并量化成4位gguf保存\n",
    "model.save_pretrained_gguf(\"autodl-tmp/model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
