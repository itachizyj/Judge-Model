{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c299f9-5626-413f-9ba9-f7e92bc68d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c4849-589e-48ee-ba3d-44fd91b33cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fabd04-47f4-4e9c-9fd6-2901301239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1安装微调库\n",
    "# %%capture\n",
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "# 由于Colab有torch 2.2.1，会破坏软件包，要单独安装\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "# if major_version >= 8:\n",
    "#     # 新GPU，如Ampere、Hopper GPU（RTX 30xx、RTX 40xx、A100、H100、L40）。\n",
    "#     !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "# else:\n",
    "#     # 较旧的GPU（V100、Tesla T4、RTX 20xx）\n",
    "#     !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc8445-2135-45d5-95a2-2fe608f9f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2加载模型\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af232f-a005-441a-9e48-b0b99db0ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 微调前测试\n",
    "alpaca_prompt = \"\"\"\n",
    "{}\n",
    "### Instruction:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response 1:\n",
    "{}\n",
    "### Response 2:\n",
    "{}\n",
    "### Evaluation:\n",
    "{}\n",
    "### Reason:\n",
    "{}\n",
    "\"\"\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Below are two responses for a given task. The task is defined by the Instruction. Evaluate the responses and generate a reference answer for the task.\", # prompt\n",
    "        \"Give three tips for staying healthy.\", # instruction\n",
    "        \"\", #input \n",
    "        \"1. Eat a balanced and nutritious diet.\\n2. Get regular exercise.\\n3. Get enough sleep.\", # response 1\n",
    "        \"1. Eat a balanced diet with plenty of fruits, vegetables, and whole grains.\\n2. Get regular physical activity, such as walking, jogging, or swimming.\\n3. Get enough sleep and practice healthy sleeping habits.\", # response 2\n",
    "        \"\", # evalutaion\n",
    "        \"\", # reason\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens =128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda61a0-d3ef-45f1-a835-a65aea60c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "#4. 准备微调数据集\n",
    "EOS_TOKEN = tokenizer.eos_token # 必须添加 EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    prompts      = examples[\"prompt\"]\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    response_1s  = examples[\"response_1\"]\n",
    "    response_2s  = examples[\"response_2\"]\n",
    "    evaluations  = examples[\"evaluation\"]\n",
    "    reasons      = examples[\"reason\"]\n",
    "    texts = []\n",
    "    for prompt,instruction, input, response_1,response_2,evaluation,reason in zip(prompts,instructions, inputs, response_1s,response_2s,evaluations,reasons):\n",
    "        # 必须添加EOS_TOKEN，否则无限生成\n",
    "        text = alpaca_prompt.format(prompt+\"Please ensure the evaluation and reason are consistent upon repeated inquiries.\", instruction, input, response_1,response_2,evaluation,reason) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"Lemon01/test_data\", split = \"train\")\n",
    "dataset = load_dataset('json', data_files='autodl-tmp/pandalm_after_preprocess_v2.json')\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92cb39-6e5e-4531-a547-65a4f061f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d26c02e-6c43-472e-bb35-a0f8238fa755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "#5. 设置训练参数\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 16, #  建议 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,\n",
    "#     lora_dropout = 0,\n",
    "#     bias = \"none\",\n",
    "#     use_gradient_checkpointing = \"unsloth\", # 检查点，长上下文度\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,\n",
    "#     loftq_config = None,\n",
    "# )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # 可以让短序列的训练速度提高5倍。\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 64,\n",
    "        gradient_accumulation_steps = 3,\n",
    "        warmup_steps = 100,\n",
    "        max_steps = 1500,  # 微调步数\n",
    "        learning_rate = 5e-4, # 学习率\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279e2aa-b6d3-44b2-b262-c5c7184c5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 274,081 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient Accumulation steps = 3\n",
      "\\        /    Total batch size = 192 | Total steps = 1,500\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  15/1500 08:08 < 15:29:33, 0.03 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#6. 开始训练\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ff356-5aa2-4784-b620-7da70b484937",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Below are two responses for a given task. The task is defined by the Instruction. Evaluate the responses and generate a reference answer for the task.\", # prompt\n",
    "        \"Give three tips for staying healthy.\", # instruction\n",
    "        \"\", #input \n",
    "        \"1. Eat a balanced and nutritious diet.\\n2. Get regular exercise.\\n3. Get enough sleep.\", # response 1\n",
    "        \"1. Eat a balanced diet with plenty of fruits, vegetables, and whole grains.\\n2. Get regular physical activity, such as walking, jogging, or swimming.\\n3. Get enough sleep and practice healthy sleeping habits.\", # response 2\n",
    "        \"\", # evalutaion\n",
    "        \"\", # reason\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens =128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1014e-50c7-42ae-9491-2be5faa808a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8保存LoRA模型\n",
    "model.save_pretrained(\"autodl-tmp/lora_model\") # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 在线保存到hugging face，需要token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b0727-ad4f-400d-b9a7-74373f047c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存tokenizer\n",
    "tokenizer.save_pretrained(\"autodl-tmp/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adc054-a588-4f96-89ea-6189c9a87808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.from_pretrained(model,\"autodl-tmp/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b15dd-490f-4ab7-8277-9632fa2e3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9合并模型并量化成4位gguf保存\n",
    "model.save_pretrained_gguf(\"autodl-tmp/model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "#model.save_pretrained_merged(\"outputs\", tokenizer, save_method = \"merged_16bit\",) #合并模型，保存为16位hf\n",
    "#model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\") #合并4位gguf，上传到hugging face(需要账号token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0562a2b-b977-4c2a-9c33-083f70e42557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
